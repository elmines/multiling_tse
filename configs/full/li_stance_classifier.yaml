seed_everything: 0 # Li's seeds were 0,112,343
trainer:
  logger:
    class_path: lightning.pytorch.loggers.CSVLogger
    init_args:
      save_dir: ./

  # No early stopping-- Li just does 5 epochs
  max_epochs: 5 

  # Li does use model checkpointing for his stance training
  # But he uses F1 score for just stance (not the targets)
  callbacks:

    # Calculate the stance F1
    - class_path: mtse.callbacks.StanceClassificationStatsCallback

    # Monitor the stance F1
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val/stance/bimacro_f1
        mode: max
        filename: "epoch={epoch:02d}-bimacro_f1={val/stance/bimacro_f1:.3f}"
        auto_insert_metric_name: false

  # These are Li's settings, for both training and inference
  # Doesn't set these for target classification
  deterministic: true
  benchmark: false

model:
  class_path: mtse.modules.LiTwoShotModule
  init_args:
    targets_path: static/li_merged_targets.txt
    stance_type: tri
data:
  class_path: mtse.data.MixedTrainingDataModule
  init_args:
    # He cut the batch size down to 64 for the stance training
    batch_size: 64

    # Notice no target_pred_path values
    # Li only uses GT targets during stance training
    stance_train_corpus:
      path: data/li_tse/raw_train_all_onecol.csv
      corpus_type: li
      transforms: [ {"class_path": "mtse.data.LiPreprocess", "init_args": {"scrub_targets": false}} ]
    target_train_corpus:
      path: data/li_tse/raw_train_all_onecol.csv
      corpus_type: li
      transforms: [ {"class_path": "mtse.data.LiPreprocess", "init_args": {"scrub_targets": true}} ]
    val_corpus:
      path: data/li_tse/raw_val_all_onecol.csv
      corpus_type: li
      transforms: [ {"class_path": "mtse.data.LiPreprocess", "init_args": {"scrub_targets": false}} ]
